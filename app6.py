{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8gYP0r_N60S",
        "outputId": "c78f7863-8619-4a29-f039-c8ccd5c8b327"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.20.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.22.4)\n",
            "Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (23.0)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.3)\n",
            "Requirement already satisfied: pandas<2,>=0.25 in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.4.4)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.1.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.27.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (5.3.0)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich>=10.11.0\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.9/dist-packages (from streamlit) (3.19.6)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.1.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.8.2)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (3.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit) (3.15.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2,>=0.25->streamlit) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (3.4)\n",
            "Collecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit) (0.1.0.post0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->altair<5,>=3.2.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (22.2.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit) (2022.7)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=1abbcd314ba9030f1773dc827d9b8a87bd570c5f2a746e5d3d07ef5ca3c2fc7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "Successfully built validators\n",
            "Installing collected packages: watchdog, validators, smmap, semver, pympler, pygments, mdurl, blinker, pydeck, markdown-it-py, gitdb, rich, gitpython, streamlit\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blinker-1.5 gitdb-4.0.10 gitpython-3.1.31 markdown-it-py-2.2.0 mdurl-0.1.2 pydeck-0.8.0 pygments-2.14.0 pympler-1.0.1 rich-13.3.2 semver-2.13.0 smmap-5.0.0 streamlit-1.20.0 validators-0.20.0 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBLUNthVNp-E",
        "outputId": "8a88d2f7-91da-4fb5-9879-90affb18f14e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:\n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "2023-03-23 16:05:59.394 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from scipy import stats\n",
        "from PIL import Image\n",
        "\n",
        "def build_model(train_data, test_data):\n",
        "    # Split the data into features and target\n",
        "       X_train, y_train = train_data.iloc[:,:-1], train_data.iloc[:,-1]\n",
        "       X_test, y_test = test_data.iloc[:,:-1], test_data.iloc[:,-1]\n",
        "\n",
        "       # Initialize the logistic regression model\n",
        "       lr = LogisticRegression()\n",
        "\n",
        "       # Train the model on the training data\n",
        "       lr.fit(X_train, y_train)\n",
        "\n",
        "       # Use the model to predict on the test data\n",
        "       y_pred = lr.predict(X_test)\n",
        "\n",
        "       # Calculate the accuracy of the model on the test data\n",
        "       accuracy = accuracy_score(y_test, y_pred)   \n",
        "       \n",
        "       return accuracy\n",
        "\n",
        "def label_encode(df, cols):\n",
        "    le = LabelEncoder()\n",
        "    for col in cols:\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "    return df\n",
        "\n",
        "def onehot_encode(df, cols):\n",
        "    ohe = OneHotEncoder()\n",
        "    for col in cols:\n",
        "        encoded = pd.DataFrame(ohe.fit_transform(df[[col]]).toarray(), columns=[f'{col}_{i}' for i in range(ohe.categories_[0].shape[0])])\n",
        "        df = pd.concat([df, encoded], axis=1)\n",
        "        df.drop(col, axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Define function to handle missing values based on the selected technique\n",
        "def handle_missing_values(df, technique):\n",
        "    if technique == 'Drop':\n",
        "        df.dropna(inplace=True)\n",
        "    elif technique == 'Fill with mean':\n",
        "        df.fillna(df.mean(), inplace=True)\n",
        "    elif technique == 'Fill with median':\n",
        "        df.fillna(df.median(), inplace=True)\n",
        "    elif technique == 'Fill with mode':\n",
        "        df.fillna(df.mode().iloc[0], inplace=True)\n",
        "        \n",
        "# Define a function to calculate the percentage of outliers\n",
        "def calculate_outlier_percentage(column):\n",
        "    q1, q3 = np.percentile(column, [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    outliers = column[(column < lower_bound) | (column > upper_bound)]\n",
        "    return len(outliers) / len(column) * 100\n",
        "\n",
        "# Define a function to check for outliers in each numerical column\n",
        "def check_outliers(df):\n",
        "    numerical_columns = df.select_dtypes(include=np.number).columns\n",
        "    outlier_percentages = {}\n",
        "    for column in numerical_columns:\n",
        "        outlier_percentages[column] = calculate_outlier_percentage(df[column])\n",
        "    return outlier_percentages\n",
        "\n",
        "# Function to remove outliers using z-score method\n",
        "def remove_outliers_zscore(data):\n",
        "    # Select a single column from the input data\n",
        "    column = data.iloc[:, 0]\n",
        "    \n",
        "    # Convert column to numeric data type\n",
        "    column_numeric = pd.to_numeric(column, errors='coerce')\n",
        "    \n",
        "    # Compute z-scores on numeric data\n",
        "    z_scores = stats.zscore(column_numeric)\n",
        "    \n",
        "    # Identify outliers\n",
        "    outliers = np.abs(z_scores) > 3\n",
        "    \n",
        "    # Remove outliers from original data\n",
        "    column[~outliers] = column_numeric[~outliers]\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Function to remove outliers using IQR method\n",
        "def remove_outliers_iqr(data):\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    filtered_data = data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "    return filtered_data\n",
        "\n",
        "\n",
        "# Title of the app\n",
        "st.title('Logistic Regression:')\n",
        "st.image(\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcStcD2Yl6mWPOd0tpEHexjKvYYZFTb9-ow5Ug&usqp=CAU\")\n",
        "\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings\n",
        "st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def main():\n",
        "  # Set up a file uploader with multiple file selection enabled\n",
        "  uploaded_file = st.file_uploader(\"Upload your input CSV file\", type=[\"csv\"])\n",
        "  if uploaded_file is not None:\n",
        "    df = pd.read_csv(uploaded_file)\n",
        "    st.success('File successfully uploaded!')\n",
        "\n",
        "    # Display raw data\n",
        "    if st.checkbox('Display Raw Data'):\n",
        "        st.write(df.head())\n",
        "    \n",
        "    # Data Types\n",
        "    if st.checkbox('Data Information'):\n",
        "        # Display dataset metadata\n",
        "        st.write('## Dataset Info')\n",
        "        st.write(f\"Number of Rows: {df.shape[0]}\")\n",
        "        st.write(f\"Number of Columns: {df.shape[1]}\")\n",
        "        st.write('---')\n",
        "        st.write('### Data Types:')\n",
        "        st.write(df.dtypes)\n",
        "        st.write('---')\n",
        "        st.write('### Descriptive Statistics:')\n",
        "        st.write(df.describe())\n",
        "        # Identify numerical and categorical columns\n",
        "        num_cols = df.select_dtypes(include=[\"int\", \"float\"]).columns.tolist()\n",
        "        st.write('### Numerical Columns:', num_cols)\n",
        "        cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "        st.write('### Categorical Columns:', cat_cols)\n",
        "        \n",
        "\n",
        "    st.title('Data Preprocessing')\n",
        "    st.subheader('Drop Unwanted Columns')\n",
        "    # Allow users to select columns to drop\n",
        "    if st.checkbox('drop'):\n",
        "        st.write(\"###### Drop Columns which are not Required\")\n",
        "        columns_to_drop = st.multiselect(\"Select columns to drop\", df.columns)\n",
        "        df = df.drop(columns_to_drop, axis=1)\n",
        "        st.success(\"Columns dropped successfully!\")\n",
        "    \n",
        "        if st.checkbox('Show dataset'):\n",
        "            # Display the filtered dataframe\n",
        "            st.dataframe(df)\n",
        "        \n",
        "    # Create the Streamlit app\n",
        "    st.subheader('Missing Values Handling')   \n",
        "    # Create a checkbox to check for missing values\n",
        "    if st.checkbox('Check missing values'):\n",
        "        missing_values = df.isnull().sum()\n",
        "        if missing_values.any():\n",
        "            st.write(missing_values)\n",
        "            st.warning('Missing Values Present')\n",
        "        else:\n",
        "            st.success('No Missing Values!')\n",
        "         \n",
        "    # Create a checkbox to handle missing values\n",
        "    if st.checkbox('Handle Missing Values'):\n",
        "        # Create a selectbox to choose the technique\n",
        "        technique = st.selectbox('Choose a technique', ('Drop', 'Fill with mean', 'Fill with median', 'Fill with mode'))\n",
        "        # Show the original dataframe\n",
        "        st.subheader('Original Dataframe')\n",
        "        st.write(df)\n",
        "        # Apply the selected technique to the dataframe\n",
        "        handle_missing_values(df, technique)\n",
        "        # Show the modified dataframe\n",
        "        st.subheader(f'Dataframe after {technique} technique')\n",
        "        st.write(df)\n",
        "\n",
        "    # Check for outliers\n",
        "    st.subheader('Checking & Handling Outliers')\n",
        "    if st.checkbox('Check for outliers'):\n",
        "        outlier_percentages = check_outliers(df)\n",
        "        for column, percentage in outlier_percentages.items():\n",
        "            st.write(f\"{column}: {percentage:.2f}% outliers\")\n",
        "        \n",
        "        # Create a checkbox to show/hide outliers\n",
        "        show_outliers = st.checkbox('Show outliers')\n",
        "        if show_outliers:\n",
        "            # Find the column with the highest outliers\n",
        "            numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            highest_outliers_col = None\n",
        "            highest_outliers_percent = 0\n",
        "    \n",
        "            for column in numerical_cols:\n",
        "                outlier_percent = calculate_outlier_percentage(df[column])\n",
        "                if outlier_percent > highest_outliers_percent:\n",
        "                    highest_outliers_col = column\n",
        "                    highest_outliers_percent = outlier_percent\n",
        "            \n",
        "            # Create a box plot of the highest outlier column\n",
        "            fig, ax = plt.subplots()\n",
        "            ax.boxplot(df[highest_outliers_col])\n",
        "            ax.set_title('Box plot of ' + highest_outliers_col)\n",
        "            ax.set_ylabel(highest_outliers_col)\n",
        "            st.pyplot(fig)\n",
        "       \n",
        "    # Create a checkbox to remove outliers using z-score method\n",
        "    if st.checkbox('Remove Outliers'):\n",
        "        remove_outliers_zscore_checkbox = st.checkbox('z-score method')\n",
        "        if remove_outliers_zscore_checkbox:\n",
        "            filtered_df = remove_outliers_zscore(df)\n",
        "            st.write('Data after removing outliers using z-score method:')\n",
        "            st.write(filtered_df)\n",
        "\n",
        "        # Create a checkbox to remove outliers using IQR method\n",
        "        remove_outliers_iqr_checkbox = st.checkbox('IQR method')\n",
        "        if remove_outliers_iqr_checkbox:\n",
        "            filtered_df = remove_outliers_iqr(df)\n",
        "            st.write('Data after removing outliers using IQR method:')\n",
        "            st.write(filtered_df)\n",
        "\n",
        "    st.subheader('Converting Categorical to Numerical')\n",
        "    cols = st.multiselect(\"Select columns to encode\", df.columns.tolist())\n",
        "    # Select encoding method\n",
        "    encode_method = st.radio(\"Select encoding method\", [\"Label Encoding\", \"One-Hot Encoding\"])\n",
        "    # Encode columns\n",
        "    if cols:\n",
        "        if encode_method == \"Label Encoding\":\n",
        "            df = label_encode(df, cols)\n",
        "            print('Label Encoded',df)\n",
        "        elif encode_method == \"One-Hot Encoding\":\n",
        "            df = onehot_encode(df, cols)\n",
        "            print('One-hot encoded data', df)\n",
        "\n",
        "    # Display encoded data\n",
        "    st.write('##### Displaying Encoded Data',df)\n",
        "\n",
        "    st.title('Data Visualization')\n",
        "\n",
        "    Plots = [\"Histogram\", \"Correlation Matrix\", \"Scatter plot\",\"Bar plot\"]\n",
        "    selected_Plot = st.selectbox(\"Choose a Plot for Visualization\", Plots)\n",
        "\n",
        "\n",
        "    if selected_Plot == \"Histogram\":\n",
        "        selected_column = st.selectbox('Select Column', df.columns)\n",
        "        plt.hist(df[selected_column])\n",
        "        st.pyplot()\n",
        "\n",
        "    elif selected_Plot == \"Correlation Matrix\":\n",
        "        corr_matrix = df.corr()\n",
        "        sns.heatmap(corr_matrix, annot=True)\n",
        "        st.pyplot()\n",
        "\n",
        "    elif selected_Plot == \"Scatter plot\":\n",
        "        x = st.selectbox('Select the x-axis for scatter plot', df.columns)\n",
        "        y = st.selectbox('Select the y-axis for scatter plot', df.columns)\n",
        "        plt.scatter(df[x], df[y])\n",
        "        st.pyplot()\n",
        "\n",
        "    elif selected_Plot == \"Bar plot\":\n",
        "        col = st.selectbox('Select the column for bar chart', df.columns)\n",
        "        plt.bar(df[col].value_counts().index, df[col].value_counts())\n",
        "        st.pyplot()\n",
        "    \n",
        "    st.title('Feature Engineering')\n",
        "    # Create selectbox for target variable\n",
        "    target_variable = st.selectbox(\"Select target variable\", df.columns)\n",
        "\n",
        "    # Use target variable in model building\n",
        "    X = df.drop(target_variable, axis=1)\n",
        "    y = df[target_variable]\n",
        "\n",
        "    # Split data\n",
        "    st.subheader('Data spltting')\n",
        "    st.write(\"#### Spliting The Data\")\n",
        "    test_size = st.slider(\"Test set size\", 0.0, 1.0, 0.2, 0.1)\n",
        "    random_state = st.slider(\"Random state\", 0, 100, 42, 1)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "    # Display encoded data\n",
        "    if st.checkbox('Training data'):\n",
        "           st.write(\"Training data:\")\n",
        "           st.write('X Train',X_train)\n",
        "           st.write('Y Train',y_train)\n",
        "\n",
        "\n",
        "\n",
        "    if st.checkbox('Testing data'):\n",
        "        st.write(\"Testing data:\")\n",
        "        st.write(\"X Test\",X_test)\n",
        "        st.write(\"Y Test\",y_test)\n",
        "\n",
        "    st.title('Model building')\n",
        "    if st.checkbox('Accuracy'):\n",
        "        train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "        accuracy = build_model(train_data, test_data)\n",
        "        st.write(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPaSBXDHNr-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}